{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pill 19b WIKI Side Quest: ECOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code and explore the problem of Error Correcting Output Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Giuseppe Onesto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to talk about a distinct and interesting ECOC approach that tries to solve some drawbacks of ECOC traditional method: the **Adaptive Error-Correcting Output Codes (AECOC)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Error-Correcting Output Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if ECOC methods represent a great solution to handle multi-class problems, they typically need some restriction to do their job:\n",
    "+ all the base classifiers are trained independently;\n",
    "+ they are directly trained on the original dataset space; they don't take into account the representation of original data, that's the idea of representational learning.\n",
    "\n",
    "The AECOC[1] tries to solve these inconvenients, by:\n",
    "+ **training the binary classifiers in a common space of data; **\n",
    "+ **learning a common subspace for all the data belonging to different classes.**\n",
    "\n",
    "Because of this, as also written in their paper, AECOC represents a reformulation of the ECOC problem as a multiple\n",
    "correlated tasks learning problem.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model\n",
    "\n",
    "Assuming that all the data have a low-dimensional subspace representation, for the l-th task, the linear predictive function is derived from the subspace as follows:\n",
    "$$\n",
    "f_l(X^l) = X^l A w_l + b_l\n",
    "$$\n",
    "\n",
    "where $w_l \\in R^d$ is the linear weight vector, $b_l \\in R$ is\n",
    "the bias parameter, $A \\in R^{D×d}$ is the linear transformation\n",
    "matrix that projects the input data onto the low-dimensional\n",
    "subspace, and d is the dimensionality of the subspace. The\n",
    "transformation matrix, A, has orthogonal columns such that $A^TA = I$.\n",
    "\n",
    "To jointly learn\n",
    "the binary classifiers and the low dimensional representation\n",
    "of data, conduct training by minimizing the following\n",
    "regularized loss function over the model parameters:\n",
    "$\\{ A, w_l, b_l | l=1, .. , L\\} $, <br>\n",
    "$ \\sum_{i=1}^L {L f_l(X^l), y^l) + \\alpha_l \\|w_l\\|^2 + \\lambda Reg(A)}$.\n",
    "\n",
    "L(·, ·) is a general loss function, Reg(A) is a regularization term with respect to A, {$α_l$}, l=1,...,L and $\\lambda $ are tradeoff parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The results obtained\n",
    "\n",
    "Below, I'm showing the results obtained by the authors on some datasets, with respect to other manyclass problem handling approaches. As you can see, AECOC usually provides very good results.\n",
    "\n",
    "![alt text](aecoc.png )\n",
    "<p style=\"text-align: center;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Zhong, Guoqiang, and Mohamed Cheriet. \"Adaptive Error-Correcting Output Codes.\" IJCAI. 2013."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
