{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pill 19 WIKI Side Quest: Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate why ensemble learning works. Focus on one of the models. For example in Bagging it is said that \"Bagging performance improvement is due to the reduction of the variace of the classifier while maintaining its bias\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### by Toni Miranda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook mix notebooks posted by David, Huang, Markos, Indra and Gonzalo, revising different ensemble methods (I have also added Random Subspace Method).\n",
    "\n",
    "*(Gonzalo) Note: I think Iris data set with 2 features is not the best example to show the advantages of ensemble learning because of  ensemble methods (like Random Forest) improve the performance for large datasets and using a lot of features due to randomization and diversity. However, this way we can see the differences of models, representing the performance in 2D. Anyway, Iris is a toy example, so the result is also good.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why ensemble learning works?\n",
    "**(David)**\n",
    "\n",
    "Ensemble learning helps improve machine learning results by combining several models. This approach allows the production of better predictive performance compared to a single model. That is why ensemble methods placed first in many prestigious machine learning competitions, such as the Netflix Competition, KDD 2009, and Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods are meta-algorithms that combine several machine learning techniques into one predictive model in order to decrease variance (bagging), bias (boosting), or improve predictions (stacking).\n",
    "\n",
    "Ensemble methods can be divided into:\n",
    "\n",
    "- **Bagging (or Bootstrapping aggregation):** Horitzontal aggregation. In bagging different models, obtained by training independent bootstrapped (resampling with replacement) elements, are combined using an aggregation technique (such as majority voting) in order to take the final decision.\n",
    "\n",
    "- **Boosting:**. Horitzontal aggregation. Each component in the aggregation depends on all the others. In Boosting algorithms each classifier is trained on data, taking into account the previous classifiers' success. After each training step, misclassified data increases its weights to emphasise the most difficult cases. In this way, subsequent learners will focus on them during their training. In this kind of techniques the two most well known approaches for linking members are the notions of residual (gradient boosting, XGBoost) or statistical resampling (adaptive boosting aka AdaBoost). \n",
    "\n",
    "- **Stacking:** Vertical aggregation. In this kind of techniques the output of one member of the ensemble is the input for the next member. We can use different model architectures and stack one on the other to improve the final performance.\n",
    "\n",
    "The basic motivation of parallel methods is to exploit independence between the base learners since the error can be reduced dramatically by averaging.\n",
    "Most ensemble methods use a single base learning algorithm to produce homogeneous base learners, i.e. learners of the same type, leading to homogeneous ensembles.\n",
    "\n",
    "There are also some methods that use heterogeneous learners, i.e. learners of different types, leading to heterogeneous ensembles. In order for ensemble methods to be more accurate than any of its individual members, the base learners have to be as accurate as possible and as diverse as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why ensemble learning?\n",
    "**(David)**\n",
    "\n",
    "To understand that why the generalization ability of an ensemble is usually much stronger than that of a single learner, there are three main reasons:\n",
    "- The first reason is that, the training data might not provide sufficient information for choosing a single best learner. For example, there may be many learners perform equally well on the training data set. Thus, combining these learners may be a better choice. \n",
    "- The second reason is that, the search processes of the learning algorithms might be imperfect.For example, even if there exists a unique best hypothesis, it might be difficult to achieve since running the algorithms result in sub-optimal hypotheses. Thus, ensembles can compensate for such imperfect search processes. \n",
    "- The third reason is that, the hypothesis space being searched might not contain the true target function, while ensembles can give some good approximation. For example, it is well-known that the classification boundaries of decision trees are linear segments parallel to coordinate axes. If the target classification boundary is a smooth diagonal line, using a single decision tree cannot lead to a good result yet a good approximation can be achieved by combining a set of decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging \n",
    "\n",
    "**(David)** \n",
    "\n",
    "Bootstrap aggregating, often abbreviated as bagging, is a involves having each model in the ensemble vote with equal weight. In order to promote model variance, bagging trains each model in the ensemble using a randomly drawn subset of the training set. As an example, the random forest algorithm combines random decision trees with bagging to achieve very high classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to reduce the variance of an estimate is to average together multiple estimates. For example, we can train M different trees on different subsets of the data (chosen randomly with replacement) and compute the ensemble:\n",
    "$$ f(x) = \\frac{1}{M} \\sum_{m=1}^{M}f_m(x)$$\n",
    "\n",
    "Bagging uses bootstrap sampling to obtain the data subsets for training the base learners. For aggregating the outputs of base learners, bagging uses voting for classification and averaging for regression.\n",
    "\n",
    "![alt text](algo_bagging.png)\n",
    "<p style=\"text-align: center;\">Figure 1. Bagging Algorithm .</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python code\n",
    "\n",
    "**(Huang)**\n",
    "\n",
    "We can study bagging in the context of classification on the Iris dataset. We can choose two base estimators: a decision tree and a k-NN classifier. Figure 1 shows the learned decision boundary of the base estimators as well as their bagging ensembles applied to the Iris dataset.\n",
    "\n",
    "The following code shows the accuracy of using a single estimator of Decision Tree and KNN, it also prints the accuracy of using Bagging classfier based on 10 estimators of Decision Tree and KNN with 0.8 subsampling of training data and 0.8 subsampling of features. Such length of estimators(10), subsamples size(0.8) and subsamples featrues(0.8) are followed by the previously David's work. Also, by default in the Sklearn tutorial page the lenght of estimator is 10 as well. More details, it can be found by [Sscikit-learn Bagging Classifier tutorial page](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlxtend'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d3bbdf5898e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmlxtend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_decision_regions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgridspec\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgridspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mlxtend'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import datasets\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, 1:3]  # we only take the first two features.\n",
    "y = iris.target\n",
    "\n",
    "#Single estimator Decision Tree \n",
    "clf_tree = tree.DecisionTreeClassifier(random_state=0)\n",
    "clf_tree.fit(X, y)\n",
    "scores_tree = cross_val_score(clf_tree, X, y)\n",
    "print(\"The accuracy of using Desicion Tree is:\",scores_tree.mean())\n",
    "\n",
    "#Single estimator KNeighbors(KNN)\n",
    "clf_KNN= KNeighborsClassifier()\n",
    "clf_KNN.fit(X, y)\n",
    "scores_KNN = cross_val_score(clf_KNN, X, y)\n",
    "print(\"The accuracy of using KNN is:\",scores_KNN.mean())\n",
    "\n",
    "# DecisionTreeClassifier 15 estimators\n",
    "clf_bagging_tree = BaggingClassifier(base_estimator=tree.DecisionTreeClassifier(random_state=0),n_estimators=15, random_state=0)\n",
    "clf_bagging_tree.fit(X,y)\n",
    "scores_bagging_tree = cross_val_score(clf_bagging_tree, X, y)\n",
    "print(\"The accuracy of using Bagging technique based on Desicion Tree is:\",scores_bagging_tree.mean())\n",
    "\n",
    "# KNeighborsClassifier 15 estimators\n",
    "clf_bagging_KNN = BaggingClassifier(KNeighborsClassifier(), n_estimators=15, random_state=0)\n",
    "clf_bagging_KNN.fit(X,y)\n",
    "scores_bagging_KNN = cross_val_score(clf_bagging_KNN, X, y)\n",
    "print(\"The accuracy of using Bagging technique based on KNN is:\",scores_bagging_KNN.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gridspec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-517a6f2907c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgridspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGridSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gridspec' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "\n",
    "fig = plt.figure(figsize=(13,9))\n",
    "\n",
    "for clf, lab, grd in zip([clf_tree, clf_KNN, clf_bagging_tree, clf_bagging_KNN], \n",
    "                         ['Decision Tree', \n",
    "                          'KNN', \n",
    "                          'Bagging Tree',\n",
    "                          'Bagging KNN'],\n",
    "                          itertools.product([0, 1], repeat=2)):\n",
    "\n",
    "    clf.fit(X, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf)\n",
    "    plt.title(lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "**(David)**\n",
    "\n",
    "In random forests, each tree in the ensemble is built from a sample drawn with replacement (i.e. a bootstrap sample) from the training set. In addition, instead of using all the features, a random subset of features is selected, further randomizing the tree.\n",
    "\n",
    "As a result, the bias of the forest increases slightly, but due to the averaging of less correlated trees, its variance decreases, resulting in an overall better model.\n",
    "\n",
    "\n",
    "![alt text](algo_random_forest.png )\n",
    "<p style=\"text-align: center;\">Figure 3. Random Forest Algorithm.</p>\n",
    "\n",
    "\n",
    "![alt text](random_forest.png )\n",
    "<p style=\"text-align: center;\">Figure 4. Example of classification of one sample by the trees on a random forest with n trees.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an extremely randomized trees algorithm randomness goes one step further: the splitting thresholds are randomized. Instead of looking for the most discriminative threshold, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows reduction of the variance of the model a bit more, at the expense of a slightly greater increase in bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of using Random Forest is: 0.914215686275\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, 1:3]  # we only take the first 3 features.\n",
    "y = iris.target\n",
    "\n",
    "rf = RandomForestClassifier(random_state=1)\n",
    "rf.fit(X,y)\n",
    "scores_rf = cross_val_score(rf, X, y)\n",
    "print(\"The accuracy of using Random Forest is:\",scores_rf.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_decision_regions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ef752cc81eb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_decision_regions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Random Forest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_decision_regions' is not defined"
     ]
    }
   ],
   "source": [
    "fig = plot_decision_regions(X=X, y=y, clf=rf)\n",
    "plt.title(\"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "**(Markos)**\n",
    "\n",
    "Stacking is an ensemble learning technique that combines multiple classification or regression models via a meta-classifier or a meta-regressor. The base level models are trained based on a complete training set, then the meta-model is trained on the outputs of the base level model as features.\n",
    "\n",
    "The base level often consists of different learning algorithms and therefore stacking ensembles are often heterogeneous. The algorithm below summarizes stacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](algo_stacking.png )\n",
    "<p style=\"text-align: center;\">Figure 5. Stacking algorithm.</p>\n",
    "\n",
    "The following example consists of k-NN, Random Forest, and Naive Bayes base classifiers whose predictions are combined by Logistic Regression as a meta-classifier. We can see the blending of decision boundaries achieved by the stacking classifier. The figure also shows that stacking achieves higher accuracy than individual classifiers and based on learning curves, it shows no signs of overfitting.\n",
    "\n",
    "Finally it is important to mention that stacking is a commonly used technique for winning the Kaggle data science competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlxtend'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-02a6be418bf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmlxtend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStackingClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mlxtend'"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "lr = LogisticRegression()\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n",
    "                          meta_classifier=lr)\n",
    "\n",
    "print('3-fold cross validation:\\n')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, sclf], \n",
    "                      ['KNN', \n",
    "                       'Random Forest', \n",
    "                       'Naive Bayes',\n",
    "                       'StackingClassifier']):\n",
    "\n",
    "    scores = model_selection.cross_val_score(clf, X, y, \n",
    "                                              cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n",
    "          % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gridspec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a7fabb126e87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgridspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGridSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gridspec' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "\n",
    "fig = plt.figure(figsize=(13,9))\n",
    "\n",
    "for clf, lab, grd in zip([clf1, clf2, clf3, sclf], \n",
    "                         ['KNN', \n",
    "                          'Random Forest', \n",
    "                          'Naive Bayes',\n",
    "                          'StackingClassifier'],\n",
    "                          itertools.product([0, 1], repeat=2)):\n",
    "\n",
    "    clf.fit(X, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf)\n",
    "    plt.title(lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "### AdaBoost \n",
    "\n",
    "**(Indra)**\n",
    "\n",
    "Adaboosting is an ensemble method that creates a strong classifier from a number of week classifiers. A week classifier performs poorly, but better than guessing.\n",
    "\n",
    "The most suited and therefore most common algorithm used with AdaBoost are decision trees with one level. Because these trees are so short and only contain one decision for classification, they are often called decision stumps. But Adaboost can be applied to any classification algorithm.\n",
    "\n",
    "Adaboost trains several weak classifiers on different random subsets of training data. After each training Adaboost assigns a weight to each traing data point, so that data points with higher weights will have a higher probability of being included in the next training subset. From each training, the examples that were misclassified, are assigned higher weight, so they will have a higher probability of being corrected in the next training. (Before the first training all data points are assigned equal probability.)\n",
    "\n",
    "![alt text](algo_boosting.png)\n",
    "<p style=\"text-align: center;\">Figure 6. Adaboost algorithm.</p>\n",
    "\n",
    "After all the classidiers are trained, they are each given weights based on their accuracies. Classifiers with 50% accuracies are assigned weight zero (because 50% accuracy is the same as guessing), classifiers with higher than 50% accuracies are assigned weights larger than zero, and classifiers with less than 50% accuracies are assigned negative weights (because they predict the opposite of correct). \n",
    "\n",
    "$w_0(x_i)=1/N$ - weights before training the first model\n",
    "\n",
    "Create classifier $h_j(x), i=j,..,K$ from the training subset.\n",
    "\n",
    "Then compute the output weight for that classifier: $\\alpha_j=\\frac{1}{2}\\ln\\big(\\frac{1-\\epsilon_{j}}{ \\epsilon_{j}}\\big)$\n",
    "\n",
    "After computing the classfier weight, we then compute weights for each training data point. \n",
    "\n",
    "$\n",
    "w_j(x_i)=\\frac{w_{j-1}(x_i) e^{- y_i \\alpha_j h_j(x_i)}}{Z_j}\n",
    "$\n",
    "\n",
    "$w_j$ is a vector of weights for each point in the training data set. \n",
    "\n",
    "$Z_j=\\sum_i w_j(x_i)$ is the sum of all weights, used to normalize the weights so that they sum up to $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b6ca76ec8f31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SAMME\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mscores_ab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mscores_ab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "ab = AdaBoostClassifier(tree.DecisionTreeClassifier(random_state=0),algorithm=\"SAMME\",n_estimators=200)\n",
    "ab.fit(X, y)\n",
    "scores_ab = cross_val_score(clf, X, y)\n",
    "scores_ab.mean()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores_ab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d50e475dacfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The accuracy of using AdaBoost technique is:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscores_ab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_decision_regions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AdaBoost\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scores_ab' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy of using AdaBoost technique is:\",scores_ab.mean())\n",
    "fig = plot_decision_regions(X=X, y=y, clf=ab)\n",
    "plt.title(\"AdaBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "**(Gonzalo)**\n",
    "\n",
    "Gradient Tree Boosting or Gradient Boosted Regression Trees (GBRT) is a generalization of boosting to arbitrary differentiable loss functions. GBRT is an accurate and effective off-the-shelf procedure that can be used for both regression and classification problems.\n",
    "\n",
    "In Gradient Boosting each model is weighted by the residual of the model trained before. \n",
    "\n",
    "*Note: Extreme gradient boosting technique, the famous classifier winner of the majority of competitions) is just a simple random projection followed by a non-linearity and a straight forward least squares optimization.*\n",
    "\n",
    "![alt text](algo_gradient_boosting.png)\n",
    "<p style=\"text-align: center;\">Figure 7. Gradient Boosting algorithm.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of using Gradient Boosting technique is: 0.90114379085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1a147c22b0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEICAYAAAB/Dx7IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFBglbhGRAqPELSJSYJS4RUQKzP8HffiFTC9zk0kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a14692240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "gb.fit(X,y)\n",
    "scores_gb = cross_val_score(gb, X, y)\n",
    "print(\"The accuracy of using Gradient Boosting technique is:\",scores_gb.mean())\n",
    "fig = plot_decision_regions(X=X, y=y, clf=gb)\n",
    "plt.title(\"Gradient Boosting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Subspace Method (RSM)\n",
    "\n",
    "**(Toni)**\n",
    "\n",
    "The Random Subspace Method (RSM) [1] randomly selects an arbitrary number of subspace from the original feature space, and build a classifier on each subspace. This randomization should create classifiers that are complementary. The combination can be carried out by simple fixed rules. Experimental evidences showed that RSM works well with feature spaces with large feature sets and redundant features. It avoids the curse of dimensionality. The concepts of RSM can be related to the theory of stochastic discrimination of Kleinberg [2].\n",
    "\n",
    "![alt text](algo_RSM.png )\n",
    "<p style=\"text-align: center;\">Figure 8. Random Subspace Method algorithm.</p>\n",
    "\n",
    "The algorithm projects each feature vector into a less dimensional subspace, by selecting m random components. RSM is similar to Bagging but instead of sampling objects, it performs a kind of feature sampling withouth replacement since it would be useless to include one feature more than once. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:\n",
    "\n",
    "[1] T. K. Ho, The random subspace method for constructing decision forests, IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 20, no. 8, pp. 832844, 1998.\n",
    "\n",
    "[2] E. M. Kleinberg, On the algorithmic implementation of stochastic discrimination, IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 22, no. 5, pp. 473490, 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
