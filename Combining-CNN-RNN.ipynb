{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pill 17 WIKI Side Quest: Advanced architectures in neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### by Toni Miranda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining CNN and RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN and RNN have been described by some of the classmates in this subquest. I would like to add on what has been said by briefly exploring when is a good idea to combine this 2 types of deep-learning models, CNN and RNN. This is on the basis of a book chapter: \"Deep Learning for Python\" by François Chollet, 2017 (Chapter 6. Deep Learning for text and sequences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Chollet (2017) two fundamental deep-learning algorithms for sequence processing are RNN and 1D CNN, the one-dimensional version of the 2D CNN explained already by classmates and typically used for computer vision. Applications of RNN and 1D CNN  algorithm combination include:\n",
    "- Document classification and timeseries classification (i.e. identifying the topic of an article or the author of a book).\n",
    "- Timeseries comparisons, such as estimating how closely related two documents are.\n",
    "- Sequence-to-sequence learning, such as decoding an English sentence into French\n",
    "- Sentiment analysis, such as classifying the sentiment of tweets or movie reviews as positive or negative (remember the Kaggle competition in this ML course?)\n",
    "- Timeseries forecasting, such as predicting the future weather at a certain location, given recent weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on 1D CNN for sequence data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolution layers introduced previously were 2D convolutions, extracting 2D patches from image tensors and applying an identical transformation to every patch. In the same way, 1D convolutions can be used, extracting local 1D patches (subsequences) from sequences as shown in the figure below (*Fig 1*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./How_1D_Conv_works.png)  \n",
    "*Fig 1.* How 1D convolution (1D CNN) works: each output timestep is obtained from a temporal patch in the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining CNNs and RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequence processing with 1-Dimension CNN (1D CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because 1D CNN process input patches independently, they aren’t sensitive to the order of the timesteps (beyond a local scale, the size of the convolution windows), unlike RNNs. To recognize longer-term patterns, it is possible to stack many convolution layers and pooling layers, but according to Chollet, that’s still a fairly weak way to induce order sensitivity because the CNN looks for patterns anywhere in the input timeseries and has no knowledge of the temporal position of a pattern it sees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One strategy to combine the speed and lightness of CNNs with the order-sensitivity of RNNs is to use a 1D convnet as a preprocessing step before an RNN. See figure below (*Fig 2*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./Combining_1D_CNN_and_RNN.png)  \n",
    "*Fig 2.* Combining a 1D CNN and an RNN for processing long sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining CNNs and RNNs to process long sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is especially beneficial when dealing with sequences that are so long they can’t realistically be processed with RNNs, such as sequences with thousands of steps. The CNN will turn the long input sequence into\n",
    "much shorter (downsampled) sequences of higher-level features. This sequence of extracted features then becomes the input to the RNN part of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key take aways on sequence processing with CNN and combining RNN and CNN:\n",
    "- In the same way that 2D CNN perform well for processing visual patterns in 2D space, 1D CNN perform well for processing temporal patterns. They offer a faster alternative to RNNs on some problems, in particular natural- language processing tasks.\n",
    "- Typically, 1D CNN are structured much like their 2D equivalents from the world of computer vision.\n",
    "- Because RNNs are extremely expensive for processing very long sequences, but 1D CNN are cheap, it can be a good idea to use a 1D CNN as a preprocessing step before an RNN, shortening the sequence and extracting useful representations for the RNN to process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple of implementation\n",
    "*Note:* It requires Keras and Tensolflow installed. https://keras.io/#installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data set can be download from here: https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Date Time\"', '\"p (mbar)\"', '\"T (degC)\"', '\"Tpot (K)\"', '\"Tdew (degC)\"', '\"rh (%)\"', '\"VPmax (mbar)\"', '\"VPact (mbar)\"', '\"VPdef (mbar)\"', '\"sh (g/kg)\"', '\"H2OC (mmol/mol)\"', '\"rho (g/m**3)\"', '\"wv (m/s)\"', '\"max. wv (m/s)\"', '\"wd (deg)\"']\n",
      "420551\n"
     ]
    }
   ],
   "source": [
    "#Inspecting the data of the Jena weather dataset\n",
    "import os\n",
    "data_dir = './'\n",
    "fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')\n",
    "f = open(fname)\n",
    "data = f.read()\n",
    "f.close()\n",
    "lines = data.split('\\n')\n",
    "header = lines[0].split(',')\n",
    "lines = lines[1:]\n",
    "print(header)\n",
    "print(len(lines))\n",
    "\n",
    "#Parsing data\n",
    "import numpy as np\n",
    "float_data = np.zeros((len(lines), len(header) - 1))\n",
    "for i, line in enumerate(lines):\n",
    "    values = [float(x) for x in line.split(',')[1:]]\n",
    "    float_data[i, :] = values\n",
    "\n",
    "#Normalizing data\n",
    "mean = float_data[:200000].mean(axis=0)\n",
    "float_data -= mean\n",
    "std = float_data[:200000].std(axis=0)\n",
    "float_data /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generator yielding timeseries samples and their targets\n",
    "def generator(data, lookback, delay, min_index, max_index,\n",
    "              shuffle=False, batch_size=128, step=6):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(\n",
    "                min_index + lookback, max_index, size=batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "        samples = np.zeros((len(rows),\n",
    "                           lookback // step,\n",
    "                           data.shape[-1]))\n",
    "        targets = np.zeros((len(rows),))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1]\n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preparing the training, validation, and test generators\n",
    "lookback = 1440\n",
    "step = 6\n",
    "delay = 144\n",
    "batch_size = 128\n",
    "\n",
    "train_gen = generator(float_data,\n",
    "                      lookback=lookback,\n",
    "                      delay=delay,\n",
    "                      min_index=0,\n",
    "                      max_index=200000,\n",
    "                      shuffle=True,\n",
    "                      step=step,\n",
    "                      batch_size=batch_size)\n",
    "val_gen = generator(float_data,\n",
    "                    lookback=lookback,\n",
    "                    delay=delay,\n",
    "                    min_index=200001,\n",
    "                    max_index=300000,\n",
    "                    step=step,\n",
    "                    batch_size=batch_size)\n",
    "test_gen = generator(float_data,\n",
    "                     lookback=lookback,\n",
    "                     delay=delay,\n",
    "                     min_index=300001,\n",
    "                     max_index=None,\n",
    "                     step=step,\n",
    "                     batch_size=batch_size)\n",
    "\n",
    "val_steps = (300000 - 200001 - lookback)\n",
    "test_steps = (len(float_data) - 300001 - lookback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.4141"
     ]
    }
   ],
   "source": [
    "#Training and evaluating a simple 1D CNN on the Jena data\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "model = Sequential()\n",
    "model.add(layers.Conv1D(32, 5, activation='relu',input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.MaxPooling1D(3))\n",
    "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(3))\n",
    "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,steps_per_epoch=500,epochs=20,validation_data=val_gen,validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Combining CNN and RNN through Mixed CNN and RNN\n",
    "\n",
    "#### Giuseppe Onesto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another very interesting and increasingly used consists in the usage of the idea of recursion, typical of RNNs, to improve standard CNNs architecture and performances.\n",
    "In literature, these approaches correspond to the so called RCNNs[1] (Recurrent CNN or Recursive CNN), and their main idea is: *\"    A prominent difference is that CNN is typically a feed-forward architecture while in the visual system recurrent connections are abundant. Inspired by this fact, we propose a recurrent CNN (RCNN) for object recognition by incorporating recurrent connections into each convolutional layer. \"*. [2]\n",
    "\n",
    "The key module of this RCNN are the recurrent convolution layers (RCL), which introduce recurrent connection into a convolution layer. With these connections the network can evolve over time though the input is static and each unit is influenced by its neighboring units. This property integrates the context information of an image, which is important for object detection, and intuitively overcome CNN problems of it. The following image shows the basic architecture of a RCNN:\n",
    "![alt text](RCNN architecture.png)\n",
    "<p style=\"text-align: center;\"></p>\n",
    "\n",
    "Of course, it shows that the inference network consists of a set of neural levels that are recursives, and this set can be of very large sizes, even every level of the CNN can be recursive, as in Liang and Hu [2] object detection RCNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RCNN For Object Recognition\n",
    "\n",
    "As said, an huge advantage of RCNN is the possibility of handling the context information of an object (eg: an image) by evolving the network nodes while the input being static. \n",
    "That explains the suitability of RCNN with problems such as image recognition; at this point, it's interesting to see more in depth the architecture of RCNN used in [2] for object recognition: the network is basically trained by backpropagation through time algorithm for recurrent networks [3];  its architecture can be understood showing and describing the below image.\n",
    "![alt text](CNNandRNNmixture.JPG)\n",
    "<p style=\"text-align: center;\"><b>Figure 2:</b> Architecture of CNN and RNN Mixture for RCNN</p>\n",
    "\n",
    "In this example, it consists of first a convolutional layer to save computations, which is followed by a max pooling layer. On top of that two RCL, one max pooling and then two additional RCL layer are used. Finally we can see one global max pooling and a softmax layer. The global max pooling layer outputs the maximum over every feature map, yielding to a feature vector that represents the image.\n",
    "\n",
    "###### The results\n",
    "\n",
    "In their paper, the authors compared their RCNN with state-of-the-art models, as shown in figure 3. RCNN-K denotes a network with K feature maps in layer one to five (eg: RCNN-96 has 96 feature maps). Table 1 compares some results on the [CIFAR-dataset](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    ". This dataset consists of 60000 color images of 32x32 pixels in ten classes. For the RCNN 50000 images were used for training and 10000 images were used for testing. The last 10000 images of the training set were used for validation of the net. The RCNN has still remarkable results compared to many other nets. It achieves, with a very low number of parameters, very good results.\n",
    "\n",
    "![alt text](table comparison.png)\n",
    "<p style=\"text-align: center;\"><b>Table 1:</b> Comparison of RCNN with respect to state-of-the-art models on the CIFAR dataset</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deeply-Recursive Convolutional Network for Image Super-Resolution\n",
    "\n",
    "The so called DRCN[4], uses the same idea as [2], by increasing the number of recursions at each layer very deeply. It's very interesting to talk about, because, as said by the authors, it tries avoiding the overfitting that would emerge by adding additional convolutional layers to a network, by increasing recursion depth at each level. Moreover, it somehow makes the model simpler to store and load, because of the less number of layers.\n",
    "\n",
    "##### The model\n",
    "*It consists of three sub-networks:  embedding, inference and reconstruction networks.\n",
    "The embedding net takes the input image (grayscale or RGB) and represents it as a set of feature maps. \n",
    "Inference net is the main component that solves the task of super-resolution. Analyzing a large image region is done by a single recursive layer. Each recursion applies the same convolution followed by a rectified linear unit.\n",
    "While feature maps from the final application of the recursive layer represent the high-resolution image, transforming them (multi-channel) back into the original image space (1 or 3-channel) is necessary. This is done by the reconstruction net.* [4]\n",
    "\n",
    "##### The results\n",
    "\n",
    "Below it's interesting to show some results on images extreme zooming with respect to other state-of-the-art methods in image super-resolution. As you can see, DRCN offers brilliant results, that are very impressive even for human eyes in image super-resolution.\n",
    "\n",
    "![alt text](drcn perf1.png)\n",
    "<p style=\"text-align: center;\"><b>Figure 3:</b> Comparison of DRCN with respect to state-of-the-art models on the super-resolution problem</p>\n",
    "\n",
    "![alt text](drcn perf2.png)\n",
    "<p style=\"text-align: center;\"><b>Figure 4:</b> Comparison of DRCN with respect to state-of-the-art models on the super-resolution problem [2]</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** References **\n",
    "\n",
    "[1] Wiest, L. (2017, February). Neural Networks - Combination of RNN and CNN. Retrieved from https://wiki.tum.de/display/lfdv/Recurrent Neural Networks - Combination of RNN and CNN <br>\n",
    "[2] Liang, Ming, and Xiaolin Hu. \"Recurrent convolutional neural network for object recognition.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015. <br>\n",
    "[3] Werbos, Paul J. \"Backpropagation through time: what it does and how to do it.\" Proceedings of the IEEE 78.10 (1990): 1550-1560. <br>\n",
    "[4] Kim, Jiwon, Jung Kwon Lee, and Kyoung Mu Lee. \"Deeply-recursive convolutional network for image super-resolution.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
